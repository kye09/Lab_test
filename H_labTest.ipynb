{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9IvWvq6ZHza",
        "outputId": "71a2a820-d327-4c59-eaa2-83aff8f82aa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/825.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.8/825.8 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/517.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.2/517.2 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --quiet timm pytorch-lightning==1.9.0 torchmetrics==0.11.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import timm\n",
        "import pytorch_lightning as L\n",
        "\n",
        "from typing import Dict, Any, List, Optional\n",
        "from glob import glob\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from torchvision.io import read_image\n",
        "from torchvision.transforms import v2 as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchmetrics import Accuracy, F1Score\n",
        "from google.colab import drive\n",
        "from google.colab import drive\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, models, transforms\n",
        "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "v84QIgm2ZVob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IebjTpYOZYVS",
        "outputId": "8da0f3dd-6749-4fcf-c87f-d17ea3ebe7a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r \"/content/drive/MyDrive/이미지 데이터\" /content/\n",
        "data_dir = \"/content/이미지 데이터\""
      ],
      "metadata": {
        "id": "TewzT7F2ZY9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing\n"
      ],
      "metadata": {
        "id": "xfum30oM-n2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, subset, transform=None):\n",
        "        self.subset = subset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.subset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.subset[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# 데이터 전처리\n",
        "transform1 = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "transform2 = transforms.Compose([\n",
        "    transforms.Resize((336, 336)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),  # 50% 확률로 좌우 반전\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # 랜덤 밝기, 대비, 채도 조정\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# ImageFolder로 데이터셋 로드\n",
        "dataset = datasets.ImageFolder(root=data_dir)\n",
        "\n",
        "# train, validation, test\n",
        "train_size = int(0.7 * len(dataset))  # 70% for training\n",
        "valid_size = int(0.15 * len(dataset))  # 15% for validation\n",
        "test_size = len(dataset) - train_size - valid_size  # 15% for testing\n",
        "\n",
        "train_dataset, valid_dataset, test_dataset = random_split(dataset, [train_size, valid_size, test_size])\n",
        "\n",
        "# 각각의 데이터셋에 다른 transform을 적용\n",
        "train_dataset1 = CustomDataset(train_dataset, transform=transform1)\n",
        "train_dataset2 = CustomDataset(train_dataset, transform=transform2)\n",
        "valid_dataset1 = CustomDataset(valid_dataset, transform=transform1)\n",
        "valid_dataset2 = CustomDataset(valid_dataset, transform=transform2)\n",
        "test_dataset1 = CustomDataset(test_dataset, transform=transform1)\n",
        "test_dataset2 = CustomDataset(test_dataset, transform=transform2)\n",
        "\n",
        "# DataLoader\n",
        "batch_size = 32\n",
        "\n",
        "train_loader1 = DataLoader(train_dataset1, batch_size=batch_size, shuffle=True, num_workers=12)\n",
        "train_loader2 = DataLoader(train_dataset2, batch_size=batch_size, shuffle=True, num_workers=12)\n",
        "valid_loader1 = DataLoader(valid_dataset1, batch_size=batch_size, shuffle=False, num_workers=12)\n",
        "valid_loader2 = DataLoader(valid_dataset2, batch_size=batch_size, shuffle=False, num_workers=12)\n",
        "test_loader1 = DataLoader(test_dataset1, batch_size=batch_size, shuffle=False, num_workers=12)\n",
        "test_loader2 = DataLoader(test_dataset2, batch_size=batch_size, shuffle=False, num_workers=12)\n"
      ],
      "metadata": {
        "id": "xQPBaFWHeDTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eva02 Model"
      ],
      "metadata": {
        "id": "8gv9FHks-QaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# eva 모델 정의\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.model = timm.create_model(\"hf_hub:timm/eva02_small_patch14_336.mim_in22k_ft_in1k\", pretrained=True)\n",
        "        # self.model = timm.create_model(\"hf_hub:timm/eva_large_patch14_336.in22k_ft_in22k_in1k\", pretrained=True)\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))  # AdaptiveAvgPool2d\n",
        "        self.clf = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.LazyLinear(1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, label=None):\n",
        "        x = self.model.forward_features(x)\n",
        "        x = self.pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        logits = self.clf(x)\n",
        "\n",
        "        loss = None\n",
        "        if label is not None:\n",
        "            loss = nn.BCEWithLogitsLoss()(logits.squeeze(-1), label.float())\n",
        "\n",
        "        probs = torch.sigmoid(logits)\n",
        "        return probs, loss\n",
        "\n",
        "\n",
        "model = CustomModel().to('cuda')\n",
        "criterion = nn.BCELoss()  # BCE Loss 사용\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.01)  # AdamW\n"
      ],
      "metadata": {
        "id": "w78r6v_1ZZCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resnet18 Model"
      ],
      "metadata": {
        "id": "rHpx2Bzq-VH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# resnet 모델\n",
        "class ResNetBinaryClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNetBinaryClassifier, self).__init__()\n",
        "        self.model = models.resnet18(pretrained=True)\n",
        "        self.model.fc = nn.Linear(self.model.fc.in_features, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return torch.sigmoid(x)\n",
        "\n",
        "model = ResNetBinaryClassifier().to('cuda')\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kc-2iiKqZkpJ",
        "outputId": "189a6878-4e5b-4fde-ade3-fb684525efbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "JrE8Yc1I-Z85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train 정의\n",
        "def train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs=100):\n",
        "    best_accuracy = 0.0\n",
        "    best_model_path = 'best_model.pth'\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        # Training Loop\n",
        "        model.train()\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to('cuda'), labels.to('cuda').float()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            probs, loss = model(images, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * images.size(0)\n",
        "\n",
        "        # Validation Loop\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for images, labels in valid_loader:\n",
        "                images, labels = images.to('cuda'), labels.to('cuda').float()\n",
        "\n",
        "                probs, loss = model(images, labels)\n",
        "                valid_loss += loss.item() * images.size(0)\n",
        "\n",
        "                preds = (probs > 0.5).float()\n",
        "                correct += (preds == labels.unsqueeze(1)).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "        train_loss = train_loss / len(train_loader.dataset)\n",
        "        valid_loss = valid_loss / len(valid_loader.dataset)\n",
        "        accuracy = correct / total\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Validation Loss: {valid_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            print(f\"Best model saved with accuracy: {best_accuracy:.4f} at epoch {epoch+1}\")\n",
        "\n",
        "    print(f\"Training complete. Best validation val_loss: {best_accuracy:.4f}\")\n",
        "\n",
        "def test_model(model, test_loader):\n",
        "    model.load_state_dict(torch.load('best_model.pth'))\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to('cuda'), labels.to('cuda').float()\n",
        "\n",
        "            probs, _ = model(images)\n",
        "            preds = (probs > 0.5).float()\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    from sklearn.metrics import f1_score, accuracy_score\n",
        "    f1 = f1_score(all_labels, all_preds)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "udTIdCAj8-Jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 실행\n",
        "train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs=100)"
      ],
      "metadata": {
        "id": "cY03DYnZ9T81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## evaluation"
      ],
      "metadata": {
        "id": "TMVwIXuv-dqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 평가를 위한 가중치 로드\n",
        "model.load_state_dict(torch.load('/content/resnet_best.pth'))\n",
        "# model.load_state_dict(torch.load('/content/eva_best.pth'))\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6tLqtdnZo5y",
        "outputId": "a28141d7-b7c3-4477-d8ac-ffa76a6c7e3b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-70-ddd3342461db>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('/content/resnet_best.pth'))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNetBinaryClassifier(\n",
              "  (model): ResNet(\n",
              "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (layer1): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer4): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "    (fc): Linear(in_features=512, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## resnet"
      ],
      "metadata": {
        "id": "yPsEjzwbZy2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader1 = DataLoader(test_dataset1, batch_size=32, shuffle=False, num_workers=12)\n",
        "\n",
        "model.load_state_dict(torch.load('/content/resnet_best.pth'))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "# 테스트 루프\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader1:\n",
        "        images, labels = images.to('cuda'), labels.to('cuda').float()\n",
        "        labels = labels.unsqueeze(1)\n",
        "\n",
        "        outputs = model(images)\n",
        "        preds = (outputs > 0.5).float()  # 예측값을 0.5 기준으로 이진화\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# F1 스코어와 정확도 계산 (이진 분류용)\n",
        "f1 = f1_score(all_labels, all_preds)\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "# 혼동 행렬 계산\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "print(\"resnet18\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"Confusion Matrix:\\n{cm}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xfa87SJbZo8P",
        "outputId": "254f4a34-70f6-421f-8405-349d17e491ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-71-e2dc99668bbf>:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('/content/resnet_best.pth'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resnet18\n",
            "Accuracy: 0.8365\n",
            "F1 Score: 0.8952\n",
            "Confusion Matrix:\n",
            "[[ 44  42]\n",
            " [ 10 222]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## eva"
      ],
      "metadata": {
        "id": "uMQ2-20mZ1b6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_loader2 = DataLoader(test_dataset2, batch_size=32, shuffle=False, num_workers=12)\n",
        "\n",
        "model.load_state_dict(torch.load('/content/eva_best.pth'))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader2:\n",
        "        images, labels = images.to('cuda'), labels.to('cuda').float()\n",
        "        labels = labels.unsqueeze(1)\n",
        "\n",
        "        outputs, _ = model(images)  # 예측값(outputs)과 손실(_)을 반환, 손실은 무시\n",
        "        preds = (outputs > 0.5).float()  # 예측값을 0.5 기준으로 이진화\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# F1 스코어, 정확도 계산 (이진 분류용)\n",
        "f1 = f1_score(all_labels, all_preds)\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "# 혼동 행렬 계산\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "# 결과 출력\n",
        "print(\"eva02\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"Confusion Matrix:\\n{cm}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsxcthjXZo-W",
        "outputId": "7773131d-a4fc-445c-f782-6af3699f2e28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-68-d663e9137ff3>:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('/content/eva_best.pth'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eva02\n",
            "Accuracy: 0.8648\n",
            "F1 Score: 0.9095\n",
            "Confusion Matrix:\n",
            "[[ 59  27]\n",
            " [ 16 216]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3DRSKXyLywBT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}